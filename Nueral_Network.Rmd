---
title: 'Neural Network'
author: "Bronson Bagwell"
date: "2024-02-07"
output: html_document
---


```{r, results='hide', warning=FALSE, message=FALSE}
library(caret)
library(neuralnet)
```

# 1. Read the data into R

```{r}
data <- read.csv("GraduateAdmission.csv")

str(data) 
names(data)
```

# 2. Do we need to convert the categorical variables into factors? Why or why not?

  We don't need to convert categorical variables into factors because neural networks can work with both qualitative and quantitative variables. However, it's essential to normalize the quantitative variables.
  
# 3. Prepare the data by using the Max-Min normalization method

```{r}
normalize <- function(x){
  return((x-min(x))/(max(x)-min(x)))
}


normdata <- as.data.frame(lapply(data,normalize))
str(normdata)
summary(normdata)
```

# 4. Split the data into a 80% training set and a 20% test set. Use set.seed(7332.01) for reproducibility

```{r}
set.seed(7332.01)
Index <- createDataPartition(normdata$Admit,p=0.8,list=FALSE)
trdata <- normdata[Index,]  # training data set 80%
tsdata <- normdata[-Index,] # testing data set 20%
head(trdata) # Gives the first 6 rows of the training data
head(tsdata) # Gives the first 6 rows of the testing data

names(normdata)
```

# 5. What are the top 3 predictors (features) that you identified using importance plot according to the MeanDecreaseGini for the Random Forest-Bagging model?
  
  1. CGPA
  
  2. GRE
  
  3. SOP
  
# 6. Find the neural network (nn) model with 1 hidden layer to predict Admit using the top 3 predictors you stated in the previous question. Report overall accuracy, misclassification, sensitivity and specificity for the test data. Use set.seed(7332.01) for reproducibility.

```{r}
set.seed(7332.01)
nnmodel <- neuralnet(Admit~ CGPA+GRE+SOP,
                     trdata,
                     hidden=1,
                     err.fct = "ce",
                     rep=5,
                     lifesign = "full",
                     linear.output = FALSE)  # Neural Network Model
# Predicting test data. 
pred_clannts <-predict(nnmodel, tsdata,rep=3) 
pred_clannts <-predict(nnmodel, tsdata,which.min(nnmodel$result.matrix[1, ]))
```
```{r}
# Gives predicted probabilities and others
head(pred_clannts) # shows the first 6 predicted probabilities from test data 
head(tsdata[1:6,])  # shows the first 6  rows of the test data

pred_classnnts<- ifelse(pred_clannts>0.5,1,0) # Assigning probabilities to classes
conf_clannts<-table(Predicted=pred_classnnts,Actual=tsdata$Admit) # confusion matrix from nnmodel
conf_clannts  # confusion Matrix from nn model

### Overall accuracy and misclasification in the test data  
ts_oacc <- sum(diag(conf_clannts))/sum(conf_clannts)  # Overall accuracy
ts_oacc  # test overall accuracy
1-ts_oacc # test misclasification

 #Sensitivity
sensitivity <- conf_clannts[2, 2] / sum(conf_clannts[2, ])
sensitivity
#Specificty
specificity <- conf_clannts[1, 1] / sum(conf_clannts[1, ])
specificity
```

  Overall Accuracy .85
  
  Misclassification .15
  
  Sensitivity .851
  
  specificity .846

# 7. Plot the nn model found in the previous question.

```{r}
plot(nnmodel,"best")
```

# 8. Attempt different parameters of the hidden layers (number of hidden layers and number of nodes). Report the model, plot, the overall accuracy, misclassification, sensitivity and specificity. Use set.seed(7332.01) for reproducibility

Two Hidden Layers

```{r}
nnmodel2 <- neuralnet(Admit~ CGPA+GRE+SOP,
                     trdata,
                     hidden=2,
                     err.fct = "ce",
                     rep=5,
                     lifesign = "full",
                     linear.output = FALSE)  # Neural Network Model
# Predicting training data. 
pred_clannts2 <-predict(nnmodel2, tsdata,rep=5) 
pred_clannts2 <-predict(nnmodel2, tsdata,which.min(nnmodel$result.matrix[1, ]))
```

```{r}
# Gives predicted probabilities and others
head(pred_clannts2) # shows the first 6 predicted probabilities from test data 
head(tsdata[1:6,])  # shows the first 6  rows of the test data

### Part II vii) ###
pred_classnnts2 <- ifelse(pred_clannts2>0.5,1,0) # Assigning probabilities to classes
conf_clannts2<-table(Predicted=pred_classnnts2,Actual=tsdata$Admit) # confusion matrix from nnmodel
conf_clannts  # confusion Matrix from nn model

### Overall accuracy and misclasification in the test data  
ts_oacc2 <- sum(diag(conf_clannts2))/sum(conf_clannts2)  # Overall accuracy
ts_oacc2  # test overall accuracy
1-ts_oacc2 # test misclasification

 #Sensitivity
sensitivity2 <- conf_clannts2[2, 2] / sum(conf_clannts2[2, ])
sensitivity2
#Specificty
specificity2 <- conf_clannts2[1, 1] / sum(conf_clannts2[1, ])
specificity2
```

  Overall Accuracy .85
  
  Misclassification .15
  
  Sensitivity .851
  
  specificity .846

```{r}
plot(nnmodel2,"best")
```

Three Hidden Layers


```{r}
nnmodel3 <- neuralnet(Admit~ CGPA+GRE+SOP,
                     trdata,
                     hidden=c(3),
                     err.fct = "ce",
                     rep=5,
                     lifesign = "full",
                     linear.output = FALSE)  # Neural Network Model
# Predicting test data. 
pred_clannts3 <-predict(nnmodel3, tsdata,rep=5) 
pred_clannts3 <-predict(nnmodel3, tsdata,which.min(nnmodel$result.matrix[1, ]))
```


```{r}
# Gives predicted probabilities and others
head(pred_clannts3) # shows the first 6 predicted probabilities from test data 
head(tsdata[1:6,])  # shows the first 6  rows of the test data

### Part II vii) ###
pred_classnnts3 <- ifelse(pred_clannts3>0.5,1,0) # Assigning probabilities to classes
conf_clannts3<-table(Predicted=pred_classnnts3,Actual=tsdata$Admit) # confusion matrix from nnmodel
conf_clannts  # confusion Matrix from nn model

### Overall accuracy and misclasification in the test data  
ts_oacc3 <- sum(diag(conf_clannts3))/sum(conf_clannts3)  # Overall accuracy
ts_oacc3  # test overall accuracy
1-ts_oacc3 # test misclasification

 #Sensitivity
sensitivity3 <- conf_clannts3[2, 2] / sum(conf_clannts3[2, ])
sensitivity3
#Specificty
specificity3 <- conf_clannts3[1, 1] / sum(conf_clannts3[1, ])
specificity3
```


  Overall Accuracy .83
  
  Misclassification .16
  
  Sensitivity .838
  
  specificity .833


```{r}
plot(nnmodel3,"best")
```

# 9. The same data was used to predict Admit in Compare the test misclassification error from the decision tree, Random Forest, Random Forest-Bagging, the logistic model and neural network model. Which model is best? 

  Decision Tree .1375
  
  Random Forest .1375
  
  Random Forest-Bagging .125
  
  Logistic Model .1125
  
  Neural Network .15
  
  Best is still the Logistic Model

# 10.

Bronson Bagwell

